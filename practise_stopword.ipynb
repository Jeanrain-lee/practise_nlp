{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practise_stopword.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPnd4FH6zALENdMGSxuH39q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeanrain-lee/practise_nlp/blob/master/practise_stopword.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABkc--aKY7cL",
        "colab_type": "text"
      },
      "source": [
        "# 불용어(Stopword) \n",
        "참조 :: https://wikidocs.net/22530 </br>\n",
        "=> 의미있는 단어 토큰만 선별하기 위해 나머지 remove (ex: 조사, 접미사 etc... => 불용어) </br>\n",
        "=> NLTK에서는 그런 100개 이상의 영단어를 불용어로 미리 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JpEHMFTe0rA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "d49d33e3-dd9c-4060-a13f-d6abb27d52d8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-N2rss5YIwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7d2bbba-3192-42e8-ca18-b88851530ff3"
      },
      "source": [
        "# NLTK에서 불용어 확인\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')[:10]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr4zqUzYeWzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK로 불용어 제거\n",
        "from nltk.tokenize import word_tokenize\n",
        "example = \"I don't know the why, but this room is super cold, what do you think?\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c25oyAqKfSvi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d7d8334c-0a73-4ab2-b51c-3ba6270fc4dd"
      },
      "source": [
        "result = []\n",
        "for w in word_tokens:\n",
        "  if w not in stop_words:\n",
        "    result.append(w)\n",
        "print(word_tokens)\n",
        "print(result) # 불용어 제거한 문장"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'do', \"n't\", 'know', 'the', 'why', ',', 'but', 'this', 'room', 'is', 'super', 'cold', ',', 'what', 'do', 'you', 'think', '?']\n",
            "['I', \"n't\", 'know', ',', 'room', 'super', 'cold', ',', 'think', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LjcFyA8lD-Q",
        "colab_type": "text"
      },
      "source": [
        "**˙ 한국어에서 불용어 제거**\n",
        "=> 토큰화 후 조사, 접속사 제거 </br>\n",
        "=> 하지만 제거 과정에서 조사 / 접속사 뿐 아니라 다른 단어들도 제거하고 싶어짐 ==> 사용자가 직접 불용어 사전 구축하게 됨\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaDLWRCpllRd",
        "colab_type": "text"
      },
      "source": [
        "# 직접 불용어 정의 후 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlZugAGdgFnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e18c3b5b-d104-41b6-c66e-fe67fa778b3d"
      },
      "source": [
        "example = \"4월이 다 되어가는데 왜 이렇게 추운지 모르겠다.. 재킷을 입고 있는데 몸이 벌벌 떨리네.\"\n",
        "stop_words = \"다 왜 이렇게 아이고 헐 있는데\"\n",
        "stop_words=stop_words.split(' ')\n",
        "word_tokens = word_tokenize(example)\n",
        "print('제거 전 :', word_tokens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제거 전 : ['4월이', '다', '되어가는데', '왜', '이렇게', '추운지', '모르겠다..', '재킷을', '입고', '있는데', '몸이', '벌벌', '떨리네', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlYA5-mUmZmo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eedc8430-6c3c-4a5a-9ebc-7b63ace6fe3a"
      },
      "source": [
        "result = [word for word in word_tokens if word not in stop_words]\n",
        "print('제거 후 :', result)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제거 후 : ['4월이', '되어가는데', '추운지', '모르겠다..', '재킷을', '입고', '몸이', '벌벌', '떨리네', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDMuDdOmm55U",
        "colab_type": "text"
      },
      "source": [
        "=> 한국어 불용어를 제거하는 더 좋은 방법은 txt나 csv로 불용어 정리 후 로드해서 사용"
      ]
    }
  ]
}