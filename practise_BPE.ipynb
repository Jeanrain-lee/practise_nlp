{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practise_BPE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7bkAubbC14/Khng90sSug"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMDHwUF8OpRc",
        "colab_type": "text"
      },
      "source": [
        "**OOV 문제**\n",
        ": 모르는 단어때문에 문제 풀이가 제대로 안됨 </br>\n",
        "**단어 분리(Subword segmenation)**\n",
        "하나의 단어는 여러 내부 단어들의 조합, 단어를 여러 개로 분리 후 단어를 이해하겠다는 전처리 작업 </br>\n",
        "=> 기계가 oov에 대해 어느 정도 대처할 수 있게 하며 주요 전처리로 사용됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drlVnuAXNP_h",
        "colab_type": "text"
      },
      "source": [
        "# BPE :: Byte Pair Encoding\n",
        "=> 대표적 단어 분리 토크나이저 </br> \n",
        "참조 :: https://wikidocs.net/22592 </br>\n",
        "1994년 나온 데이터 압축 알고리즘, 그 후 자연어처리의 단어 분리 알고리즘으로 응용됨 </br>\n",
        "=> 글자(character)단위에서 점점 단어 집합을 만들어내는 Bottom up 방식의 전개 </br>\n",
        ": 단어들을 글자 또는 유니코드 단위로 단어 집합을 만든 후 가장 많이 증장하는 유니그램을 하나의 유니그램으로 통합\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJgF9bqzNK9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re, collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XjWl-c-Tz0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8-JsQCQgGqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = {'l o w </w>' :5, #</w> : 단어의 맨 끝에 사용, 각 단어를 글자단위 분리\n",
        "         'l o w e r </w>' :2,\n",
        "         'n e w e s t</w>':6,\n",
        "         'w i d e s t</w>':3}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WORkjurUgVP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_stats(vocab):\n",
        "  pairs = collections.defaultdict(int)\n",
        "  for word, freq in vocab.items():\n",
        "    symbols = word.split()\n",
        "    for i in range(len(symbols)-1):\n",
        "      pairs[symbols[i], symbols[i+1]] += freq\n",
        "  return pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLSS5FzZvX5n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "cc108e9e-276b-4f39-d65b-01a890589e57"
      },
      "source": [
        "get_stats(vocab)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {('e', 'r'): 2,\n",
              "             ('low', 'e'): 2,\n",
              "             ('r', '</w>'): 2,\n",
              "             ('wid', 'est</w>'): 3})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFBSRIfujlBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_vocab(pair, v_in):\n",
        "  v_out = {}\n",
        "  # re.escape : 영문자, 숫자가 아닌 문자에 대해 백슬러시 추가\n",
        "  # 메타문자(특수 문자 기호) 포함 문자열을 정규식 변경 가능\n",
        "  bigram = re.escape(' '.join(pair))\n",
        "  p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "  for word in v_in:\n",
        "    w_out = p.sub(''.join(pair), word)\n",
        "    v_out[w_out] = v_in[word]\n",
        "  return v_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l04PQSfbj-2X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "1dc30a80-ca80-466c-b239-8b74c347fead"
      },
      "source": [
        "for i in range(epochs):\n",
        "  pairs = get_stats(vocab)\n",
        "  best = max(pairs, key=pairs.get)\n",
        "  vocab = merge_vocab(best, vocab)\n",
        "  print(best)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('e', 's')\n",
            "('es', 't</w>')\n",
            "('l', 'o')\n",
            "('lo', 'w')\n",
            "('n', 'e')\n",
            "('ne', 'w')\n",
            "('new', 'est</w>')\n",
            "('low', '</w>')\n",
            "('w', 'i')\n",
            "('wi', 'd')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S6f9FGFsWME",
        "colab_type": "text"
      },
      "source": [
        "˙ 실무에서는 단어 분리를 위해 구글의 센텐스피스(sentencepiece) 사용 </br>\n",
        "=> 사전 토큰화 작업 없이 전처리 적용 안 된 데이터에 바로 단어 분리 토큰화 수행, 언어에 종속되지 않는다"
      ]
    }
  ]
}